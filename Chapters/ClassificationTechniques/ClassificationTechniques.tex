\chapter{Classification techniques}
\label{ch:ClassificationTechniques}

\section{Introduction}
This chapter deals with the statistical foundations of species distribution modelling. Since it is impractical to list all the available methods that are used we restrict ourselves to the most popular or fundamental ones. In the last 10 years a lot of research has focused on the performance of these different algorithms \parencite[e.g.][]{elith*_novel_2006,segurado_evaluation_2004}. The results of these studies were taken into account when the methods that are used were selected.

\section{Presence-absence data}
\label{sec:PresenceAbsenceData}

In this section some important and often used methods to classify binary data are reviewed. First of all, the outcome, $y_i$, of observation $i$ indicates whether a species occurs, $y_i = 1$, or is absent, $y_i=0$. We denote the vector of explanatory variables as $\bm{X}$. The general form of the models used in this section is:
\begin{equation}
\label{FundEq}
P(Y=1|\bm{X} = \bm{x}) = f(\bm{x}; \bm{\gamma}).
\end{equation}
In this representation $f(\cdot;\cdot)$ is a function parametrized by $\bm{\gamma}$. The main differences between the techniques introduced below are the functional form of $f(\cdot;\cdot)$ and the loss function that is minimized.

\subsection{Logistic regression}
Perhaps the most fundamental modelling technique for binary data is logistic regression. In logistic regression the log odds ratio of the probability of an occurrence is modelled as a linear function of the covariates. Hence, the model can be depicted as \[\log \left( \frac{P(Y=1|\bm{X} = \bm{x})}{P(Y=0|\bm{X} = \bm{x})} \right)= \gamma_0 \bm{x}^t \bm{\gamma}.\]
It is easy to show that this model can be written in the form used in Equation \ref{FundEq}. More specifically, if we define $\expit(\cdot) = \frac{\exp(\cdot)}{1+\exp(\cdot)}$ we get \[f(\bm{x};\bm{\gamma}) = \expit(\gamma_0 + \mathbf{x}^t\bm{\gamma}).\] Usually the coefficients of a logistic regression model are obtained by using maximum likelihood estimation (MLE). Obtaining the MLE $\widehat{\bm{\gamma}}$ corresponds with solving the following maximization problem:
\[\widehat{\bm{\gamma}} = \argmax_{\bm{\gamma}} \sum_{i=1}^{N} \left\lbrace y_i \log{f(\bm{x}_i;\bm{\gamma})}  + (1-y_i)\log{f(\bm{x}_i;\bm{\gamma})}  \right\rbrace.\] 
When one multiplies this log-likelihood function by minus one we get a loss function that is often called the cross-entropy. For more information about logistic regression and numerical optimization techniques for obtaining the MLE we refer to \cite{agresti_categorical_2013} and \cite{mccullagh_generalized_1999}. \\

The main advantages of logistic regression models are that they are relatively simple to implement, interpret, etc. This simplicity is also its greatest disadvantage. In particular, when modelling the distribution of a species there is often no a priori knowledge of the shape of the response curves.

\subsection{Generalized additive models}
\label{sec:GAM}
In standard logistic regression a linear systematic component is used. It is easy to extend logistic regression models to include non-linear systematic components. However, the functional form of the log odds ratio might not be known by the researcher and hence a non-parametric (or semi-parametric) modelling technique can be useful. When the distribution of the outcome belongs to the exponential family generalized additive models (GAMs) are one possible class of non-parametric (or semi-parametric) models. In the case of a Bernoulli distribution the resulting GAM is sometimes called an additive logistic regression model and has the form:
\begin{equation}
\label{GAMs}
\log \left( \frac{P(Y = 1|\bm{X} = \bm{x} )}{P(Y=0|\bm{X} = \bm{x})} \right) = \gamma_0 + f_1(x_{1}) + \dots + f_p(x_{p}).
\end{equation}
In this representation the $f_k(\cdot)$'s are certain smooth functions. Again one can write this model in the form of Equation \ref{FundEq}:
\[f(\bm{x};\bm{\gamma}) = \gamma_0 + f_1(x_{1}) + \dots + f_p(x_{p})\] 
There is a multitude of popular ways to specify the $f_k(\cdot)$'s \parencite{wood_generalized_2006, hastie_generalized_1990}. We will follow \cite{wood_generalized_2006} and \cite{wood_gams_2002} and focus on using cubic smoothing splines to represent the $f_k(\cdot)$'s. \\

GAMs are most often fitted by using MLE. In order to restrict the ``wiggliness'' of the smoothing functions in model \ref{GAMs} one can add a penalization term to the likelihood function. An example of such a ``wiggliness'' penalty is:
\[\sum_{j=1}^p \lambda_j \int_{x_{j(1)}}^{x_{j(n)}} \{f^{(2)}_j(x) \}^2dx.\]
In this penalization term the $x_{j(1)}$ (resp.\ $x_{j(n)}$) is the smallest (resp.\ largest) value of the $j$'th covariate. Furthermore, it can be shown that, in a class of sufficiently smooth functions, the minimizer of
\[- \sum_{i=1}^{N} \left\lbrace y_i \log{f(\bm{x}_i;\bm{\gamma})}  + (1-y_i)\log{f(\bm{x}_i;\bm{\gamma})} \right\rbrace + \sum_{j=1}^p \lambda_j \int_{x_{j(1)}}^{x_{j(n)}} \{f_j^{(2)}(x)\}^2 dx\]
is a natural cubic spline with knots at the $n$ covariate values. The two limiting cases, $\lambda = 0$ and $\lambda = \infty$, are interesting. If $\lambda = 0$ an interpolating spline is optimal, while when $\lambda \to \infty$ the solution converges to the linear logistic regression solution. Finally, fitting GAMS with natural cubic splines as smoothing functions is, compared to most other smoothers used in GAMS, computationally efficient.\\

In practice we have to select the penalization parameters, this is usually done by using cross-validation. Because the fitting procedures for GAMs are often computationally intensive usually the closely related generalized cross-validation (GCV) is used \parencite{wood_gams_2002}. Finally, since $\lambda = \infty$ still allows a first order fit, it can be interesting to perform additional model selection. Although variable selection is the topic of Chapter \ref{ch:ReducingTheNumberOfExplanatoryVariables} we mention that it is possible to introduce an extra penalization term that leads to an automatic variable selection technique \parencite{marra_practical_2011}, i.e.\ might remove a predictor from the model. This approach is implemented in the \textsc{mgcv} R package \parencite{mgcv}. \\

Finally, up till now only univariate splines have been considered, if interaction terms are to be included one can use e.g.\ thin plate splines. However, using multidimensional splines usually results in a computationally intensive fitting procedure. Finally, for a more applied review of the use of GAMs in species distribution modelling we refer to \cite{guisan_generalized_2002}.

\subsection{Artificial neural networks}
Artificial neural networks (ANNs) are a non-linear modelling technique. We refer to \cite{bishop_neural_1995} for an introduction to the general methodology and some of the technical details. The terminology used in the ANN literature is slightly different than in the standard statistical literature. More particularly, the explanatory variables are usually called the input features. Furthermore, an ANN consists of so-called ``layers'' of ``neurons'. The first layer is called the input layer and consists of one neuron for each variable. In each successive layer the output of the corresponding neurons is the result of applying an activation function, $g(\cdot)$, to a linear combination of the values from the previous layer. The coefficients of these linear combinations are called the weights of the ANN. These weights are the parameters one can tune. The process of feeding the values of the previous layer into the next is repeated up to the last layer which is called the output layer. The layers that are neither the input nor output layer are called hidden layers. A graphical representation of an ANN with one hidden layer can be found in Figure \ref{fig:chClassificationTechniques:ANN}.\\

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.3]{VectorGraphics/ANN.png}
\caption{\label{fig:chClassificationTechniques:ANN}Visualization of a feed-forward neural network with one hidden layer.}
\end{figure}

From here on we will denote the vector of all the weights as $\bm{\gamma}$. As usual, the estimated weights are obtained by minimizing a loss function. When ANNs are applied to classification problems often either the squared loss, $L(\bm{y};\bm{\gamma}) = \sum_i (y_i - f(\bm{x}_i;\bm{\gamma}))^2$ or the cross-entropy, $L(\bm{y};\bm{\gamma}) = - \sum_i \left[ y_i \log \{ f(\bm{x}_i;\bm{\gamma})\} + (1-y_i)\log \{f(\bm{x}_i;\bm{\gamma})\}\right]$ is used. We will use the cross-entropy since it is closely related to the likelihood of a Bernoulli distribution and hence more in line with the other techniques. To minimize the loss criterion, often backpropagation \parencite{rumelhart_learning_1986} is used in combination with a numerical optimization algorithm, e.g.\ steepest descent. \\

It is easy to see that logistic regression can be seen as an ANN with: no hidden layers, the expit function as the activation function of the output layer, and the cross-entropy loss.\\

The biggest strength of ANNs is that, under some regularity constraints, they can approximate any continuous function arbitrarily well \parencite{hornik_multilayer_1989}. Some disadvantages include:
\begin{itemize}
\item Selecting an optimal number of layers and neurons is far from trivial.
\item The loss-function often has multiple local minima.
\item Different numerical optimization methods often lead to different solutions.
\item Fitting large neural network architectures can be computationally infeasible.
\item The backpropagation algorithm cannot be used in combination with non-differentiable penalty functions, e.g.\ the lasso penalty (see Section \ref{sec:Lasso}).
\item The fitted parameters can be sensitive to the initial weights.
\item The obtained model seems like a ``black-box'', i.e. there is often no easy way to interpret the parameters and the effect of different predictors.
\end{itemize}
Some of these disadvantages can be, partially, overcome by using e.g.\ weight decay, averaging networks, early stopping, pruning, \dots\ Weight decay is also referred to as $L_2$ regularization and is described in Section \ref{sec:L2Regularization}. Averaging networks boils down to using different sets of initial values, fitting the same network structure for each set, and then combining these into one model \parencite{ripley_pattern_2009}. The \textsc{caret} package \parencite{caret} provides an implementation of averaged neural networks based upon the \textsc{nnet} package \parencite{nnet}.


\subsection{Tree based methods}
In this section some tree based methods are introduced.  The section starts with a short explanation of decision trees. Afterwards boosting is introduced as a method to deal with some of the shortcomings of decision trees.

\subsubsection{Decision trees}
\label{sec:DecisionTrees}
Tree based methods are a class of algorithms that partition the input space into rectangular regions. The same predicted value is then assigned to all observations within a certain region. In the context of SDMs we can interpret this as partitioning the environmental space into rectangles. Each of these rectangles is then labelled as being part of the niche or not. To obtain such rectangles we start by splitting the input space into two regions along one variable. The variable and the value that is used to obtain this split is selected in such a way that the loss function of interest is minimized. In the following steps either the algorithm stops if some stopping criterion is met or the obtained sub-regions are split into smaller sub-regions. After the tree is grown usually a winner takes all approach is applied to obtain the label for each region. Thus, if a region contains mainly presences its predicted class will be the presence class, otherwise it will be labelled as a region containing absences. Finally, the number of nodes $J$ of a tree is defined as the number of splits $+1$. A visualization of a partitioning obtained bys using a classification tree is given in Figure \ref{fig:chClassificationTechniques:CART}. \\

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{VectorGraphics/CART.png}
\caption{\label{fig:chClassificationTechniques:CART}Visualization of a classification tree in a two dimensional input space.}
\end{figure}

Unless a stopping criteria is specified this approach would lead to over-fitting. More particularly we would end up with as many regions as there are observations. Hence, the algorithm is usually stopped when no new splits can be found that decrease the loss by some pre-specified amount. Another possible stopping criterion is to stop the algorithm once a certain number of splits is reached. As usually we opt for using the cross-entropy as the loss function. Finally, we note that there are many variations to the algorithm sketched above.

The most important advantages of decision trees include:
\begin{itemize}
\item Complex interaction effects can easily be modelled.
\item Small decision trees are easily visualized.
\item Decision trees usually perform relatively well even when no variable selection was applied.
\item The idea underlying decision trees is quite intuitive.
\item Decision trees are invariant under monotonic transformations of the predictors.
\end{itemize}

Some of the disadvantages of using decision trees include:
\begin{itemize}
\item Decision trees usually have a large variance
\item Categorical variables with a lot of classes can lead to computational problems.
\end{itemize}

\subsubsection{Boosting}
\label{sec:Boosting}
In order to combat the large variance of decision tree boosting was introduced. The overview of boosting presented in this section is largely based on \cite{elith_working_2008} and \cite{friedman_additive_2000}. \cite{elith_working_2008} gave an applied working guide to boosted regression trees while \cite{friedman_additive_2000} gave a theoretical explanation of boosted regression trees.\\

An ensemble of different classifiers is sometimes useful to obtain improved classifiers. This is what is done in boosted classification trees. Boosting can be described as creating a sequence of models such that the $i$'th model focusses on correctly classifying observations that were misclassified by the $i-1$ previous models. The corresponding algorithm then takes the following form:\\
\begin{enumerate}
\item Construct a classifier.
\item Classify the observations.
\item Assign large weights to wrongly classified observation and vice versa for correctly classified observations. 
\item Fit a classifier on the weighted data-set.
\item Create a new classifier by adding the the new and old classifiers, usually the new classifier is shrunken by multiplying it with a regularization parameter.
\item Stop if some stopping criteria is met, otherwise use the new classifier obtained in step 5. and repeat from step 2. onwards.
\end{enumerate}

In order to avoid over-fitting one can use a new subsample of the full data-set in each iteration of the algorithm. When combining boosting with classification trees it is recommended that the subsamples have a sample size in between $0.5$ and $0.75$ times the size of the full dataset \parencite{elith_working_2008}.\\

Another popular ensemble method that is used in combination with decision trees is the Random Forest (RF) method. In most cases RFs perform slightly worse than boosted classification trees \parencite{hastie_elements_2009} and we will not consider them in this thesis.\\

A disadvantage of boosted classification trees is that they are not as easily visualized / interpreted as normal classification trees. Secondly, there are quite a few tuning parameters, namely the depth of the trees $J$, the regularization term, and the number of trees. However, \cite{hastie_elements_2009} note that using $J \in \{4,\dots,8\}$ usually leads to the best performing models, additionally they observed that the specific value of $J$ in this set has little effect on the performance of the classifier. Hence, cross validation will be used to obtain optimal values for the learning rate and the number of trees.\\

Finally, although we described boosting for classification trees the same idea can be readily applied to other classification algorithms. Furthermore, boosted classification trees seem to include a form of internal variable selection, i.e.\ the performance of this method usually doesn't degrade a lot when irrelevant predictors are present. We will use the implementation provided by the \textsc{gbm} package \parencite{gbm}.

\section{Presence-only data}
\label{sec:PresenceOnlyData}
Instead of having access to presence-absence data it happens quite often that only presence data is available. Since the previously described classification techniques need binary data they can not immediately be used with presence-only data. In Section \ref{sec:IPP} the inhomogeneous Poisson process (IPP) is introduced. Presence-background classification is introduced in Section \ref{sec:ClassificationWithPseudoAbsences}. Finally, in Section \ref{sec:MaximumEntropyModeling} Maximum Entropy \parencite[MaxEnt;][]{phillips_maximum_2006,phillips_modeling_2008} is concisely described. \\ 

Although we will not consider it we note that there is another interesting way to use regression models in combination with presence-only data. \cite{ward_presence-only_2009} used the EM algorithm \parencite{dempster_maximum_1977} in combination with regression models. Although this leads to an elegant and rigorously motivated method of fitting regression models, the prevalence of the species needs to be known, or estimable, which is nearly never the case.

\subsection{Poisson point processes}
\label{sec:IPP}
First of all, all the measure theoretic machinery involved in point processes is blatantly ignored in this section. We denote the study area of interest by $S$. Usually $S$ corresponds to an area preserving projection of the earth and therefore our focus is on $S\subset \mathbb{R}^2$. A point process is a random variable $\bm{X}$ that consists of points $u_i \in S$, hence $\bm{X} =  (u_1,\ldots,u_n)$. In the presence-only scenario the $\bm{X}$ variable equals the locations where an occurrence was reported. One popular point process is the inhomogeneous Poisson point process. Before giving a non-rigorous definition of the IPP we define the intensity of a point process as a function $\lambda(\cdot): S \to \mathbb{R}_+$ for which $\int_B \lambda(u)du < \infty,\; \forall$ bounded $B \subset S$. The random variable $\bm{X}$ is said be an IPP if $\forall$ bounded $ B \subset S$:
\begin{enumerate}
\item The number of observed points within $B$, $N(B) = \# (X \cap B)$, follows a Poisson distribution with mean $\Lambda(B) = \int_B \lambda(u)du.$
\item Given the number of observed points within $B$, $N(B)$, the elements of $X$ within $B$ are i.i.d. distributed with density $\frac{\lambda(u)}{\Lambda(B)}.$
\end{enumerate}
The first part of the definition implies that the the expected number of observations in a small area surrounding a point is approximately equal to the intensity, assuming it is a continuous function, multiplied by size of the area. \\

In order to obtain a parametric model of an IPP the intensity is usually modelled as a function of the covariates $\bm{z}(u)$ observed at the location $u$. A popular model is the log linear model: 
\begin{equation*}
\label{eq:Intensity}
 \lambda(u) = \exp (\gamma_0  + \bm{\gamma}^t\bm{z}(u) ). 
\end{equation*}
It is interesting to note that the intercept term, $\gamma_0$, only affects the expected number of points but not the configuration of the points in $S$. When we use $C$ to denote a rest term that is independent of the parameters, the likelihood function becomes:
\begin{equation*}
\label{eq:IPPlikelihood}
\begin{aligned}
l(\bm{x};\bm{\gamma}) &= \log \left( \prod_{i=1}^n \frac{\lambda(u_i)}{\Lambda(S)} \frac{ \Lambda(S)^n \exp (-\Lambda(S))}{n!}\right)  \\ 
&= \sum_{i=1}^n \left\lbrace \gamma_0 + \bm{\gamma}^t\bm{z}(u_i) \right\rbrace - \int_S \exp (\bm{\gamma}^t\bm{z}(u)) du  + C.
\end{aligned}
\end{equation*}
The difficulty of using MLE in combination with IPP models is approximating the integral. For an overview of different methods to obtain the MLEs and a more complete treatment of spatial point processes we refer to \cite{moller_modern_2007}.

\subsection{Classification with pseudo-absences}
\label{sec:ClassificationWithPseudoAbsences}
\todo[inline]{discuss effect of using PCA on background vs occurrence data}
One of the older techniques to deal with presence-only data consists of generating so-called pseudo-absence or background points. The original motivation of this technique is that by uniformly sampling $n_0$ points in the geographical space $S$ one obtains a representation of the available habitat in the study region \parencite{pearce_modelling_2006}. The available habitat is then contrasted with the locations where a presence was observed by using standard classification methods. In practice one has to decide on the number of pseudo-absences that should be used and whether or not the pseudo-absences should be weighted \parencite{barbet-massin_selecting_2012}.\\

Another, more rigorous, way to justify the pseudo-absence method is by using the IPP model. \cite{warton_poisson_2010} were the first to note the connections between fitting an IPP model and fitting logistic regression models with pseudo-absences. \cite{fithian_finite-sample_2013} further refined these equivalences and also considered the situation when the model is misspecified. We follow the derivation given by \cite{fithian_finite-sample_2013}.\\

First of all, we condition on the number of presence points, $n_1$, and the number of background points, $n_0$, and view this as a case-control sampling scheme. It is clear that the probability of outcome $Y_i$ being a presence point equals $P(Y_i = 1) = \frac{n_1}{n_0 + n_1}$ and hence $P(Y_i = 0) = \frac{n_0}{n_0 + n_1}$. Now suppose that the presence points are generated by a IPP with intensity $\lambda_1(u) = \exp(\gamma_0 + \bm{\gamma}^t\bm{z}(u))$. Since the background points are generated uniformly over $S$ the intensity of this process is $\lambda_0(u) \propto 1.$ Conditional on $n_1$ and $n_0$ it is now easy to derive the expression of a logistic model:
\begin{equation}
\begin{aligned}
P(Y_i = 1 | U = u) & = \frac{f(u|Y_i = 1) P(Y_i = 1)}{P(Y_i = 1) f(u|Y_i = 1) + P(Y_i = 0)f(u|Y_i = 0) } \\[0.5ex]
 & = \frac{\frac{\lambda_1(u)}{\Lambda_1(S)}\frac{n_1}{n_0 + n_1}}{\frac{\lambda_1(u)}{\Lambda_1(S)}\frac{n_1}{n_0 + n_1} + \frac{\lambda_0(u)}{\Lambda_0(S)}\frac{n_0}{n_0 + n_1}} 
 = \frac{\frac{\lambda_1(u) n_1 \int_S du }{\Lambda_1(S)\lambda_0(u) n_0}}{\frac{ \lambda_1(u) n_1 \int_S du}{\Lambda_1(S) n_0} + 1} \\[0.5ex]
 & = \frac{\exp \left( \gamma_0 + \log \left( \frac{n_1 \int_S du }{n_0\Lambda_1(S)}\right) + \bm{\gamma}^t\bm{z}(u) \right)}{\exp \left( \gamma_0 + \log \left( \frac{n_1 \int_S du }{n_0\Lambda_1(S)}\right) + \bm{\gamma}^t\bm{z}(u) \right) + 1}
\end{aligned}
\end{equation}
This derivation implies that, with the exception of the intercept, the coefficients of the IPP model can be estimated by using the standard fitting procedures for logistic regression. Furthermore, it shows that combining logistic regression together with back-ground data is quite a natural approach to the presence-only problem. The fact that the intercept of the IPP model cannot be obtained is not problematic. As we saw in Section \ref{sec:IPP} the intercept does not affect the configuration of the observed points within $S$. Instead it basically rescales the expected number of observed presences. Since the number of observed presences is partially determined processes of which we have no data, e.g.\ the sampling intensity, the intercept is usually not of interest,


\subsection{Maximum Entropy modelling}
\label{sec:MaximumEntropyModeling}
Perhaps the most popular method to create models from presence-only data is Maximum Entropy modelling. \cite{phillips_maximum_2006} considered a gridded study area $\mathcal{X}$. An occurrence of a species then occurs in a cell based manner, hence instead of an exact location in $S$ it is only known that the species was present inside the corresponding cell in $\mathcal{X}$. They then went on to find a distribution $\pi(x): \mathcal{X} \to \mathbb{R}$ which minimizes the entropy: 
\[ - \sum_{x\in\mathcal{X}} \pi(x) \log \{\pi(x)\},  \]
under the constraint that the observed mean of the features, $\bm{z}(x) \in \mathbb{R}^p$, is ``close'' to the expected mean. If we denote the set of cells for which an occurrence was observed by $\mathcal{X}_1$ the constraint is:
\[\left| \sum_{x_i \in \mathcal{X}_1} \bm{z}(x_i)  - \sum_{x \in \mathcal{X}} \pi(x)\bm{z}(x) \right| < \bm{\lambda}, \quad \bm{\lambda} \in \mathbb{R}^p_+,\]
where the inequality is applied component wise. It can be shown that the solution to this problem has the following form:
\[\pi(x) \propto \exp (\bm{\gamma}^t\bm{z}(x)), \quad \bm{\gamma} \in \mathbb{R}^p.\]
It is interesting to note that this is exactly the form of a log linear IPP model. \cite{renner_equivalence_2013} showed that when the grid size becomes small the MaxEnt solution converges towards the penalized IPP solution. The standard implementation of MaxEnt uses as features a combination of products between covariates, hinge features, step functions, quadratic terms and linear terms \parencite{phillips_modeling_2008}. Hence, in the end MaxEnt modelling is equivalent with presence-background logistic regression in an extended covariate space. \\

\section{Taking the scale hierarchy into account}
\label{sec:TakingTheScaleHierarchyIntoAccount}
In Section \ref{sec:SpatialScale} the influence of spatial scale was discussed. There have been some attempts to incorporate this hierarchical structure into the classification techniques. E.g.\ \cite{pearson_modelling_2004} combined two models, one for coarse scale processes and one that introduces fine grain variables. More specifically their approach was as follows: \\
\begin{enumerate}
\item An initial model is obtained by solely using climate variables as predictors.
\item The predicted values are saved as a new variable.
\item The predicted values and remote sensed variables are combined and used as the input for a second model.
\item The predicted values of the second model are the final predictions.
\end{enumerate}

Graphically this can be depicted as in Figure \ref{fig:chClassificationTechniques:HierarchicalClassification}.\\

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{VectorGraphics/HierarchicalClassification.png}
\caption{\label{fig:chClassificationTechniques:HierarchicalClassification}Visualization of the hierarchical model.}
\end{figure}

This hierarchical model has since been applied in combination with e.g.\ stacked SDMs \parencite{cord_remote_2014}. However, from a statistical point of view this approach is not well motivated. The main drawback of this approach is that interaction effects between the climate and remotely sensed variables cannot be taken into account. Furthermore, since there is usually quite some correlation between the climate and remotely sensed variables model selection is hampered. For example, if a remotely sensed variable is fundamental in determining the niche and highly correlated with a climate variable that is not as relevant for defining the niche the climate variable will usually end up in the model instead of the remotely sensed variable. Because of these drawbacks this approach will not be investigated.







